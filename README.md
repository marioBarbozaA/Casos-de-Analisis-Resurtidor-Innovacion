# README - Casos-de-Analisis-Resurtidor-Innovacion

---

# üîπ Overview del Proyecto

Este proyecto tiene como objetivo realizar un an√°lisis de inventario utilizando la arquitectura Medallion (Bronze, Silver) en Azure Databricks, partiendo de un archivo Excel suministrado por Walmart. El enfoque fue:

- Cargar datos en capa **Bronze** (sin transformaciones).
- Realizar limpieza y validaciones en capa **Silver**.
- Generar an√°lisis estad√≠sticos y visualizaciones para entender el desempe√±o de **In Stock** y **Dispersi√≥n**.

**Tecnolog√≠as utilizadas:**
- Microsoft Excel
- Azure Databricks
- PySpark
- Python (Matplotlib, Pandas)

---

# üí° Caso 1: Preparaci√≥n de datos y An√°lisis Exploratorio

## 1.1 Procesamiento Inicial en Excel
- Se abri√≥ el archivo `Base_Prueba.xlsx`.
- Se identific√≥ que el tab √∫nico relevante era `Base_prueba`.
- Se calcularon las columnas adicionales:
  - **In Stock**: `(Combinaciones - Oust) / Combinaciones`
  - **Dispersi√≥n**: `Tiendas sin Inventario / Tiendas con Inventario`
- Se agregaron f√≥rmulas en Excel para todo el dataset.
- Se export√≥ √∫nicamente la hoja `Base_prueba` como un nuevo archivo `.csv` llamado `base_prueba_bronze.csv`.

## 1.2 Creaci√≥n de Infraestructura en Azure
- **Resource Group** creado: `rg-walmart-prueba-dev-eus2`
- **Workspace de Databricks** creado: `adb-walmart-prueba`
- **Cluster** creado: `cluster-walmart-prueba`
  - Tipo: Single Node
  - Runtime: 12.2 LTS (Scala 2.12, Spark 3.3.2)
  - Worker Type: Se intent√≥ `Standard_DS4_v2`, pero debido a error de "stockout" en `eastus2`, se cambi√≥ exitosamente a `Standard_DS3_v2`.

## 1.3 Subida de datos a DBFS
- El archivo `base_prueba_bronze.csv` se subi√≥ exitosamente al path `/FileStore/tables/`.
- Se verific√≥ que el archivo estaba disponible usando `dbutils.fs.ls("/FileStore/tables/")`.

## 1.4 Carga de Bronze Layer
- Se cre√≥ el notebook `nb-bronze-carga-base-prueba`.
- Se carg√≥ el archivo `.csv` utilizando PySpark con:
  ```python
  spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/FileStore/tables/base_prueba_bronze.csv")
  ```
- Se despleg√≥ exitosamente `df_bronze`.

## 1.5 Limpieza de datos (Silver Layer)
- En el mismo notebook (de forma excepcional para esta prueba), se realiz√≥ la limpieza de datos:
  - Eliminaci√≥n de registros con `Combinaciones <= 0` o nulos en `In Stock`, `Dispersi√≥n`, `Tiendas con Inventario` y `Tiendas sin Inventario`.
- Se cre√≥ un nuevo DataFrame `df_silver`:
  ```python
  df_silver = df_bronze.filter((col("Combinaciones") > 0) & (col("In Stock").isNotNull()) & (col("Dispersi√≥n").isNotNull()) & (col("Tiendas con Inventario").isNotNull()) & (col("Tiendas sin Inventario").isNotNull()))
  ```
- Se cre√≥ una vista temporal `silver_base_prueba`.

## 1.6 An√°lisis Estad√≠stico
- Se realizaron res√∫menes y estad√≠sticas usando PySpark:
  - Estad√≠sticas globales (`describe`) para `Combinaciones`, `Oust`, `In Stock` y `Dispersi√≥n`.
  - Promedio de `In Stock` y `Dispersi√≥n` por **Pa√≠s**.
  - Promedio de `In Stock` y `Dispersi√≥n` por **Formato**.
  - Top 10 Categor√≠as con **peor In Stock**.

## 1.7 Visualizaciones
- Se crearon los siguientes gr√°ficos:
  - Gr√°fico de barras: **In Stock Promedio por Pa√≠s**.
  - Gr√°fico de barras: **Dispersi√≥n Promedio por Formato**.
  - Gr√°fico de barras horizontal: **Top 10 Categor√≠as con Peor In Stock**.

---

# üöß Problemas encontrados y soluciones aplicadas

| Problema | Soluci√≥n Aplicada |
|:--|:--|
| Stockout de VM `Standard_DS4_v2` en EastUS2 | Cambiar a `Standard_DS3_v2` para cluster |
| No aparec√≠a "Data" en Databricks para subir CSV | Se subi√≥ manualmente usando `Upload` en `/FileStore/tables/` |
| Intento de creaci√≥n de tabla autom√°tica en Databricks | Se evit√≥, prefiriendo carga limpia por PySpark |
| Valores nulos encontrados en "Tiendas con Inventario" y "Tiendas sin Inventario" | Se filtraron en la limpieza de `df_silver` |

---

# üîπ Caso 2: An√°lisis de In Stock

## Actividades realizadas
- Se analiz√≥ el comportamiento de In Stock a nivel global, pa√≠s, formato y categor√≠a.
- Se identificaron desviaciones respecto a la meta de 97.5%.
- Se construyeron gr√°ficos de apoyo para visualizaci√≥n.
- Se elaboraron conclusiones y recomendaciones por nivel de an√°lisis.

## Principales hallazgos
- El In Stock promedio global fue de 96.36%, por debajo de la meta.
- Guatemala lider√≥ en In Stock, pero no cumpli√≥ meta.
- DISCOUNT fue el formato con mejor desempe√±o.
- Cosm√©ticos (D59) fue la categor√≠a m√°s afectada.

## Recomendaciones
- Intervenir categor√≠a de cosm√©ticos.
- Replicar buenas pr√°cticas de DISCOUNT.
- Implementar alertas autom√°ticas para categor√≠as con In Stock < 95%.

---

# üîπ Caso 3: An√°lisis de Dispersi√≥n

## Actividades realizadas
- Se analiz√≥ la Dispersi√≥n global, por pa√≠s y por formato.
- Se verific√≥ el cumplimiento de la meta de 9%.
- Se elaboraron visualizaciones de Dispersi√≥n promedio.

## Principales hallazgos
- La Dispersi√≥n promedio global fue de 12.37%, superando la meta.
- Nicaragua fue el √∫nico pa√≠s que cumpli√≥ la meta de Dispersi√≥n.
- HYPERMARKET present√≥ la mayor Dispersi√≥n entre los formatos.

## Recomendaciones
- Replicar las pr√°cticas de Nicaragua y DISCOUNT.
- Redise√±ar log√≠stica en HYPERMARKET y BODEGA.
- Establecer alertas de Dispersi√≥n >15% y auditar tiendas en Costa Rica.

---

# üîπ Caso 4: Script SQL

Este caso se desarrollar√° durante la entrevista. A√∫n no ha sido abordado en esta fase.

---

# üîπ Caso 5: Modelo de Almacenamiento de Datos

## Objetivo
Dise√±ar un modelo de almacenamiento robusto que permita:
- Construir un historial confiable de datos.
- Facilitar el an√°lisis de tendencias y evoluci√≥n en el tiempo.
- Evitar duplicidades.
- Permitir auditor√≠a del origen y carga de los datos.

## Caracter√≠sticas clave
- Uso de `fecha_corte` y `fecha_carga` para diferenciar el momento del dato y su ingreso al sistema.
- Campo `source` para trazabilidad.
- Clave primaria compuesta para prevenir duplicados.
- ID t√©cnico `id_inventario` para referencia interna.

## Estructura propuesta: `fact_inventario_historial`

| Campo                   | Tipo         | Descripci√≥n                                               |
|------------------------|--------------|-----------------------------------------------------------|
| `id_inventario`        | UUID / String| ID t√©cnico del registro                                  |
| `pais`                 | String       | C√≥digo del pa√≠s (ej.: CR, GT, HN)                         |
| `formato`              | String       | Tipo de tienda (DISCOUNT, BODEGA, etc.)                   |
| `categor√≠a`            | String       | Categor√≠a de producto                                     |
| `mes`                  | Integer      | Mes del dato (ej.: 1 a 12)                                |
| `combinaciones`        | Integer      | Total de combinaciones registradas                        |
| `oust`                 | Integer      | Total de quiebres (Oust)                                  |
| `tiendas_con_inventario` | Integer    | Tiendas que ten√≠an el producto                            |
| `tiendas_sin_inventario` | Integer    | Tiendas que no ten√≠an el producto                         |
| `in_stock`             | Decimal(5,4) | Porcentaje de In Stock                                    |
| `dispersi√≥n`           | Decimal(5,4) | Porcentaje de Dispersi√≥n                                  |
| `fecha_corte`          | Date         | Fecha a la que corresponde el dato                        |
| `fecha_carga`          | Timestamp    | Fecha en que se carg√≥ el registro al sistema              |
| `source`               | String       | Fuente del dato (ej.: `upload_excel`, `pipeline_autom`)   |

## Diagrama del modelo

![Diagram_model](https://github.com/user-attachments/assets/dcd8f5d2-8cdd-4ddb-9715-2ee2bef09652)

## Beneficios del modelo
- Hist√≥rico limpio y auditable.
- Preparado para dashboards y tendencias.
- Flexible y extensible.
- 
---

# üîπ Caso 6: Modelo de Almacenamiento Semanal en Nube

## Escenario
- Los usuarios cargan semanalmente un archivo Excel.
- El contenido debe almacenarse en una tabla en la nube (por ejemplo, BigQuery).
- Se deben cumplir los principios de historial, auditor√≠a y control de duplicados definidos en el Caso 5.

## Propuesta de Soluci√≥n

### Flujo de procesamiento

1. **Carga del archivo Excel** en un bucket de almacenamiento en la nube (Google Cloud Storage o Azure Blob Storage).
2. **Trigger autom√°tico** mediante Cloud Functions o Azure Functions al detectar un nuevo archivo.
3. **Pipeline de procesamiento** utilizando Databricks Autoloader o Dataflow:
   - Validaci√≥n de formato y datos.
   - Prevenci√≥n de duplicados.
   - Agregado de metadata (source, fecha de carga, usuario).
4. **Carga de datos** en BigQuery en la tabla `fact_inventario_historial` (modo append).
5. **Consumo BI** desde Power BI, Looker o Google Data Studio.

### Tecnolog√≠as propuestas

| Etapa | Tecnolog√≠a |
|:---|:---|
| Almacenamiento de archivos | Google Cloud Storage (GCS) o Azure Blob Storage |
| Automatizaci√≥n de ingesta | Cloud Functions / Azure Functions |
| Procesamiento | Databricks (PySpark) o Google Dataflow |
| Almacenamiento de datos | BigQuery |
| Consumo de datos | Power BI, Looker, Google Data Studio |

### Beneficios de esta Arquitectura
- **Historial completo** gracias a `fecha_corte` y `fecha_carga`.
- **Prevenci√≥n de duplicados** con validaciones de claves naturales.
- **Auditor√≠a** de procesos de carga mediante campos de fuente y usuario.
- **Alta disponibilidad** para equipos de BI.
- **Escalabilidad** y **automatizaci√≥n** del proceso de ingesta.

### Representaci√≥n del flujo

```
Usuario
  ‚Üì
Carga Excel ‚Üí Bucket en Storage (GCS / Blob Storage)
  ‚Üì
Trigger autom√°tico (Cloud Function / Azure Function)
  ‚Üì
Pipeline de procesamiento (Databricks / Dataflow)
  ‚Üì
Tabla BigQuery: fact_inventario_historial
  ‚Üì
Dashboards BI (Power BI / Looker / Data Studio)
```

---

_Elaborado por: Mario Barboza_

_Proyecto: Prueba de Casos de Analisis Resurtidor Innovacion_

